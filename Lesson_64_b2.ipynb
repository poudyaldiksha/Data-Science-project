{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poudyaldiksha/Data-Science-project/blob/main/Lesson_64_b2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lesson 64: Decision Tree Algorithm - Introduction"
      ],
      "metadata": {
        "id": "bMlgZ5PaYPYV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7yJKX9f1G9W"
      },
      "source": [
        "\n",
        "\n",
        "Let's look into the decision tree algorithm in detail.\n",
        "\n",
        "When a decision tree is built, the main task is to select the **best attribute** from all the features to identify the root node and split it further. This selection of best attributes is known as the **Attribute Selection Measure (ASM)**. With the help of an ASM, we can easily select the best features for the respective nodes of a decision tree.\n",
        "\n",
        "There exists a number of ASM techniques, but the most common are:\n",
        "\n",
        "1. Gini Index\n",
        "2. Information Gain\n",
        "\n",
        "Let's go through them in detail one by one.\n",
        "\n",
        "#### Gini Index\n",
        "\n",
        "A gini index (aka gini impurity) is a measure of the impurity in a given node. Gini index measures the degree or probability of a particular variable being wrongly classified when it is randomly chosen. The Gini index varies between $0$ and $1$, where\n",
        "\n",
        "- $0$ indicates the purity of classification (as in leaf node) and\n",
        "\n",
        "- $1$ indicates the target elements are randomly distributed across various classes.\n",
        "\n",
        "Mathematically, gini index is defined as:\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{Gini Index} = 1 - \\sum_{i = 1} ^{N}(p_{i})^2\n",
        "\\end{equation}\n",
        "\n",
        "where\n",
        "\n",
        "- $p_i$ is the probability of an object being classified to a particular class\n",
        "\n",
        "- $N$ is the total number of samples\n",
        "\n",
        "When we build a decision tree, we prefer **<u>splitting</u>** the nodes across that  feature that has the least gini index value at the root node.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "**Splitting** is the process of dividing a node into two or more sub-nodes. For each split, two determinations are made:\n",
        "\n",
        "the feature used for the split called the splitting feature, and\n",
        "\n",
        "the set of values for the splitting feature (which are split between the left child node and the right child node), called the split point.\n",
        "\n",
        "The split is based on a particular criterion such as gini index and information gain (for classification) or sums of squared errors (for regression) from the entire data set. Splitting continues until a leaf node is constructed.\n",
        "\n",
        "**Parent** and **Child Nodes:** A node that is divided into sub-nodes is called a parent node and the resulting sub-nodes are the child of a parent node.\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "Once we find out the root node we proceed again with gini index calculations for the sub-node(s) formed after splitting. This process is repeated continuously until we reach to all leaf node(s) for every possible split."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Creating a sample DataFrame\n",
        "ex_data = {\n",
        "    'customer_id': [1, 2, 3, 4],\n",
        "    'mobile_cost': [10000, 14000, 30000, 18000],\n",
        "    'target': ['low', 'low', 'high', 'low']\n",
        "}\n",
        "\n",
        "ex_df = pd.DataFrame(ex_data)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(ex_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y50rAObYbs3_",
        "outputId": "2d06635e-fa30-47d2-84fe-dba5d3585afb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   customer_id  mobile_cost target\n",
            "0            1        10000    low\n",
            "1            2        14000    low\n",
            "2            3        30000   high\n",
            "3            4        18000    low\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "se7n5dIlYIeO"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhl6EvrA1evY"
      },
      "source": [
        "#### Decision Tree Algorithm - Gini Index Calculation\n",
        "\n",
        "In order to better understand the steps in the formation of a decision tree using **gini index** as an attribute selection measure, consider a very small snippet from the credit card defaulter dataset:\n",
        "\n",
        "Gender | Education | Default\n",
        "--- | --- | ---\n",
        "`Male` | `Unknown` | `Yes`\n",
        "`Female` | `High School` | `No`\n",
        "`Male` | `High School` | `No`\n",
        "`Female` | `Graduate` | `Yes`\n",
        "`Male` | `Graduate` | `No`\n",
        "\n",
        "**Note:** This is a very small subset of the entire data. Hence, do not try to infer the main decision tree using this dataset. The decision tree resulting from this dataset is meant only for understanding the gini index calculation process.\n",
        "\n",
        "Let us try to find out the decision tree for classifying the defaulter based on the 2 attributes listed in the table above.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_VrrIbNk6oo"
      },
      "source": [
        "#### Determining a Root Node\n",
        "\n",
        "There are 2 features (Education and Gender) in the above table. This implies that we have two candidates for root nodes: (a) Gender (b) Education\n",
        "\n",
        "\n",
        "**Splitting Root Node wrt Gender**\n",
        "\n",
        "Let us first calculate the gini index for the **Gender** column.\n",
        "\n",
        "There are 2 instances of `Female` value in the **Gender** column: one of them is a `defaulter` (represented by `Yes` or `1`) and another is a `non-defaulter` (represented by\n",
        "`No` or `0`)\n",
        "\n",
        "- So, the probability of a `Female` credit card client being a `defaulter` is $p_1 = \\frac{1}{2}$\n",
        "\n",
        "- Similarly, the probability of a `Female` credit card client being a `non-defaulter` is $p_2 = \\frac{1}{2}$\n",
        "\n",
        "Let the gini index of the `Female` value be $f$.\n",
        "\n",
        "Therefore, the gini index of $f$ is given by\n",
        "\n",
        "$$f = 1 - \\sum_{i = 1}^{N} (p_{i})^2$$\n",
        "\n",
        "Here, $N = 2$ because there are only two probabilities, i.e., $p_1$ and $p_2$. So, the value of $i$ also goes from $1$ to $2$.\n",
        "\n",
        "\\begin{align}\n",
        "\\therefore f &= 1 - \\sum_{i = 1}^{N} (p_{i})^2 \\\\\n",
        "&= 1 - ( p_1^2 + p_2^2 )\n",
        "\\end{align}\n",
        "\n",
        "On substituting the values of $p_1 = \\frac{1}{2}$ and $p_2 = \\frac{1}{2}$ in the above equation, we get\n",
        "\n",
        "\\begin{align}\n",
        "&= 1 - \\left( \\left(\\frac{1}{2}\\right)^2 + \\left(\\frac{1}{2}\\right)^2 \\right) \\\\\n",
        "&= 1 - \\frac{1}{2} \\\\\n",
        "\\Rightarrow f &= 0.5\n",
        "\\end{align}\n",
        "\n",
        "Similarly, there are 3 instances of `Male` value in the **Gender** column: one of them is a `defaulter` (represented by `Yes` or `1`) and two of them are `non-defaulter` (represented by\n",
        "`No` or `0`)\n",
        "\n",
        "- So, the probability of a `Male` credit card client being a `defaulter` is $p_1 = \\frac{1}{3}$\n",
        "\n",
        "- Similarly, the probability of a `Male` credit card client being a `non-defaulter` is $p_2 = \\frac{2}{3}$\n",
        "\n",
        "Let the gini index of the `Male` value be $m$.\n",
        "\n",
        "Therefore, the gini index of $m$ is given by\n",
        "\n",
        "$$m = 1 - \\sum_{i = 1}^{N} (p_{i})^2$$\n",
        "\n",
        "Here, $N = 2$ because there are only two probabilities, i.e., $p_1$ and $p_2$. So, the value of $i$ also goes from $1$ to $2$.\n",
        "\n",
        "\\begin{align}\n",
        "\\therefore m &= 1 - \\sum_{i = 1}^{N} (p_{i})^2 \\\\\n",
        "&= 1 - ( p_1^2 + p_2^2 )\n",
        "\\end{align}\n",
        "\n",
        "On substituting the values of $p_1 = \\frac{1}{3}$ and $p_2 = \\frac{2}{3}$ in the above equation, we get\n",
        "\n",
        "\\begin{align}\n",
        "&= 1 - \\left( \\left(\\frac{1}{3}\\right)^2 + \\left(\\frac{2}{3}\\right)^2 \\right) \\\\\n",
        "&= 1 - \\frac{5}{9} \\\\\n",
        "\\Rightarrow m &= 0.444\n",
        "\\end{align}\n",
        "\n",
        "The next step is to calculate the **weighted gini index** value for the **Gender** column in the above table.\n",
        "\n",
        "**Weighted Gini Index (WG)**\n",
        "\n",
        "The weighted gini index is defined as the weighted average of the gini indices of all the unique values in a column (or feature). In the case of the above table, the weighted gini index is the weighted average of the gini indices of the `Female` and `Male` values.\n",
        "\n",
        "Mathematically, WG is defined as\n",
        "\n",
        "$$\\text{WG} = (\\text{proportion of Female instances in the Gender column}) \\times (\\text{Gini Index of Female}) + (\\text{proportion of Male instances in the Gender column}) \\times (\\text{Gini Index of Male})$$\n",
        "\n",
        "In general, the WG of a column (or feature) is given as\n",
        "\n",
        "$$\\text{WG} = (\\text{proportion of } \\text{Value}_1 \\text{ instances}) \\times (\\text{Gini Index of Value}_1) + (\\text{proportion of } \\text{Value}_2 \\text{ instances}) \\times (\\text{Gini Index of Value}_2) + \\dots + (\\text{proportion of } \\text{Value}_k \\text{ instances}) \\times (\\text{Gini Index of Value}_k)$$\n",
        "\n",
        "where $k$ is the total number of unique values in a column (or feature).\n",
        "\n",
        "**Note:** *Weighted Gini Index is taken as the final Gini Index value for a column*\n",
        "\n",
        "If we split a decision tree the **Gender**, then we will get 2 nodes: `Male` and `Female`. The total number of samples in the **Gender** column is 5.\n",
        "\n",
        "Out of 5 data points in the **Gender** column, 2 are `Female`, and 3 are `Male`.\n",
        "\n",
        "- So, the proportion of `Female` instances = $\\frac{2}{5}$\n",
        "\n",
        "- And, the proportion of `Male` instances = $\\frac{3}{5}$\n",
        "\n",
        "Therefore, the $\\text{WG}$ for the **Gender** column is\n",
        "\n",
        "\\begin{align}\n",
        "\\text{WG} = \\left(\\frac{2}{5}\\right) \\times 0.5 + \\left(\\frac{3}{5}\\right) \\times 0.444 = 0.4664\n",
        "\\end{align}\n",
        "\n",
        "This means if we split the tree using the **Gender** feature as the root node, we get a Weighted Gini Index of $0.4664$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwChaIZ9sX_j"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4jkepTSrhQD"
      },
      "source": [
        "**Splitting Root Node wrt Education**\n",
        "\n",
        "Let us now calculate the gini index for the **Education** column.\n",
        "\n",
        "There are 2 instances of `Graduate` in the **Education** column: one of them is a `defaulter` and another is a `non-defaulter`.\n",
        "\n",
        "- So, the probability of a `Graduate` credit card client being a `defaulter` is $p_1 = \\frac{1}{2}$\n",
        "\n",
        "- Similarly, the probability of a `Graduate` credit card client being a `non-defaulter` is $p_2 = \\frac{1}{2}$\n",
        "\n",
        "Let the gini index for `Graduate` value be $g$.\n",
        "\n",
        "Therefore, gini index of $g$ is given by\n",
        "\n",
        "$$g = 1 - \\sum_{i = 1}^{N} (p_{i})^2$$\n",
        "\n",
        "Here, $N = 2$ because there are only two probabilities, i.e., $p_1$ and $p_2$. So, the value of $i$ also goes from $1$ to $2$.\n",
        "\n",
        "\\begin{align}\n",
        "\\therefore g &= 1 - \\sum_{i = 1}^{N} (p_{i})^2 \\\\\n",
        "&= 1 - ( p_1^2 + p_2^2 )\n",
        "\\end{align}\n",
        "\n",
        "On substituting the values of $p_1 = \\frac{1}{2}$ and $p_2 = \\frac{1}{2}$ in the above equation, we get\n",
        "\n",
        "\\begin{align}\n",
        "&= 1 - \\left( \\left(\\frac{1}{2}\\right)^2 + \\left(\\frac{1}{2}\\right)^2 \\right) \\\\\n",
        "&= 1 - \\frac{1}{2} \\\\\n",
        "\\Rightarrow g &= 0.5\n",
        "\\end{align}\n",
        "\n",
        "Similarly, there are 2 instances of `High School` value in the **Education** column and both are `non-defaulter`.\n",
        "\n",
        "- So, the probability of a `High School` graduate credit card client being a `defaulter` is $p_1 = \\frac{0}{2} = 0$\n",
        "\n",
        "- Similarly, the probability of a `High School` graduate credit card client being a `non-defaulter` is $p_2 = \\frac{2}{2} = 1$\n",
        "\n",
        "Let the gini index of the `High School` value be $h$.\n",
        "\n",
        "Therefore, gini index of $h$ is given by\n",
        "\n",
        "$$h = 1 - \\sum_{i = 1}^{N} (p_{i})^2$$\n",
        "\n",
        "Here, $N = 2$ because there are only two probabilities, i.e., $p_1$ and $p_2$. So, the value of $i$ also goes from $1$ to $2$.\n",
        "\n",
        "\\begin{align}\n",
        "\\therefore h &= 1 - \\sum_{i = 1}^{N} (p_{i})^2 \\\\\n",
        "&= 1 - ( p_1^2 + p_2^2 )\n",
        "\\end{align}\n",
        "\n",
        "On substituting the values of $p_1 = \\frac{1}{3}$ and $p_2 = \\frac{2}{3}$ in the above equation, we get\n",
        "\n",
        "\\begin{align}\n",
        "&= 1 - \\left( 0^2 + 1^2 \\right) \\\\\n",
        "&= 1 - 1 \\\\\n",
        "\\Rightarrow h &= 0\n",
        "\\end{align}\n",
        "\n",
        "Similarly, there is 1 instance of `Unknown` value in the **Education** column and it is a `defaulter`.\n",
        "\n",
        "So, the probability of an `Unknown` graduate credit card client being a `defaulter` is $p_1 = \\frac{1}{1} = 1$\n",
        "\n",
        "Let the gini index of the `Unknown` value be $u$.\n",
        "\n",
        "Therefore, the gini index of $u$ is given by\n",
        "\n",
        "$$u = 1 - \\sum_{i = 1}^{N} (p_{i})^2$$\n",
        "\n",
        "Here, $N = 1$ because there is only one probability, i.e., $p_1$. So, the value of $i$ also goes from $1$ to $1$.\n",
        "\n",
        "\\begin{align}\n",
        "\\therefore u &= 1 - \\sum_{i = 1}^{N} (p_{i})^2 \\\\\n",
        "&= 1 - p_1^2\n",
        "\\end{align}\n",
        "\n",
        "On substituting the value of $p_1 = \\frac{1}{3}$ in the above equation, we get\n",
        "\n",
        "\\begin{align}\n",
        "&= 1 - \\left( 0^2 + 1^2 \\right) \\\\\n",
        "&= 1 - 1 \\\\\n",
        "\\Rightarrow u &= 0\n",
        "\\end{align}\n",
        "\n",
        "The next step is to calculate the **weighted gini index** value for the **Education** column.\n",
        "\n",
        "Out of 5 data points in the **Education** column, 1 is `Unknown`, 2 are `Graduate`, and 2 are `High School`.\n",
        "\n",
        "- So, the proportion of `Graduate` instances = $\\frac{2}{5}$\n",
        "\n",
        "- The proportion of `High School` instances = $\\frac{2}{5}$\n",
        "\n",
        "- And, the proportion of `Unknown` instances = $\\frac{1}{5}$\n",
        "\n",
        "Therefore, the $\\text{WG}$ for the **Education** column is\n",
        "\n",
        "\\begin{align}\n",
        "\\text{WG} = \\left(\\frac{2}{5}\\right) \\times 0.5 + \\left(\\frac{2}{5}\\right) \\times 0 + \\left(\\frac{1}{5}\\right) \\times 0 = 0.2\n",
        "\\end{align}\n",
        "\n",
        "This means if we split the tree using the **Education** feature as the root node, we get a Weighted Gini Index of $0.2$\n",
        "\n",
        "\n",
        "*Hence, we will choose <u>Education</u> as the <u>root node</u> as it produces the lowest Gini Index value.*\n",
        "\n",
        "The decision tree using the **Education** column as the root node can be drawn as:\n",
        "\n",
        "<img src = \"https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/images/decision-tree-example-4.png\">\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "**Pure Node:** A node that has sample(s) of only 1 label is called a pure node. The splitting operation continues in a decision tree until all the sub-nodes obtained after splitting are pure nodes.\n",
        "\n",
        "**Impure Node:** Any node that has sample(s) that are a combination of 2 or more labels is an impure node. Other than leaf nodes, all nodes of a decision tree are impure nodes.\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "After this, there is only 1 feature left i.e. **Gender** with 1 defaulter and 1 non-defaulter. The split of the **Gender** column is straightforward, hence, no need to calculate the gini for splitting the decision tree further.\n",
        "\n",
        "**Note:** This tree is obtained from a very small subset of the entire data, hence might not look like the original decision tree made from the entire dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec8GB4Kv1eom"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUxr0nz71eka"
      },
      "source": [
        "#### Activity 1: Decision Tree Algorithm - Information Gain\n",
        "\n",
        "Let's now learn another ASM technique called information gain that is meant to select the best attribute or feature to split a decision tree. However, information gain is based on the entropy of a dataset. Let us first understand the concept of entropy.\n",
        "\n",
        "\n",
        "**Entropy** signifies the randomness in a dataset. It is a metric that measures impurity. A less impure node requires less information to describe it and a more impure node requires more information. Information theory is a measure to define this degree of disorganisation in a system known as entropy. If a sample is completely homogeneous (having the same type of values), then the entropy is zero and if the sample is equally divided (50—50%), it has an entropy of one. The entropy of a dataset $S$ is calculated as follows.\n",
        "\n",
        "\\begin{align}\n",
        "\\text{Entropy}(S) = - \\sum_{i=1}^{N}p_{i} \\times \\log_{2}(p_{i})\n",
        "\\end{align}\n",
        "\n",
        "where $p_i$ is the probability of an object classified as a class (or label).\n",
        "\n",
        "**Information Gain (IG)** is the measurement of changes in entropy value after the splitting/segmentation of a dataset wrt on an attribute.\n",
        "\n",
        "IG is calculated as:\n",
        "\n",
        "\\begin{align}\n",
        "\\text{IG} = \\text{Entropy(S)} - [\\text{Weighted average of Entropy of each unique value of a feature}]\n",
        "\\end{align}\n",
        "\n",
        "or simply\n",
        "\n",
        "\\begin{align}\n",
        "\\text{IG} = \\text{Entropy}(S_{b}) - \\text{Entropy}(S_{a})\n",
        "\\end{align}\n",
        "\n",
        "where,\n",
        "- $S$ is a dataset\n",
        "- $S_b$ is a dataset before split and $S_a$ is the dataset after the split\n",
        "\n",
        "When we build a decision tree, we prefer splitting the nodes across the attribute/feature which has maximum information gain after splitting through the root node.\n",
        "\n",
        "Once we find out the root node we proceed again with the gini index calculations for the sub-node(s) formed after splitting. This process is repeated continuously until we reach all the leaf node(s) for every possible split."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5Z3Mx_49axo"
      },
      "source": [
        "\n",
        "#### Decision Tree Algorithm - Information Gain Calculation\n",
        "\n",
        "In order to better understand the steps for the formation of a decision tree, let us take yet another very small snippet from the credit card defaulter dataset and understand the process:\n",
        "\n",
        "Gender | Education | Default\n",
        "--- | --- | ---\n",
        "`Male` | `Unknown` | `Yes`\n",
        "`Male` | `High School` | `No`\n",
        "`Female` | `High School` | `No`\n",
        "`Male` | `Graduate` | `No`\n",
        "`Female` | `Graduate` | `Yes`\n",
        "\n",
        "**Note:**  The above table is a very small subset of the entire dataset, hence do not try to infer the main decision tree using this dataset.\n",
        "\n",
        "Let us try to create a decision tree to identify credit card defaulters wrt Education and Gender.\n",
        "\n",
        "#### Determining Root Node\n",
        "\n",
        "There are two possibilities for root nodes: (a) Gender (b) Education\n",
        "\n",
        "First, we will calculate the entropy of the above table. We know that entropy is given by\n",
        "\n",
        "\\begin{align}\n",
        "\\text{Entropy} = -\\sum_{i=1}^{N}p_{i}\\times \\log_{2}(p_{i})\n",
        "\\end{align}\n",
        "\n",
        "In the above table, we have 5 samples in total out of which 2 are defaulters and 3 non-defaulters.\n",
        "\n",
        "Let $p_1$ and $p_2$ be the probability of defaulters and non-defaulters respectively.\n",
        "\n",
        "- So, the probability of defaulters $p_1 = \\frac{2}{5}$\n",
        "\n",
        "- And, the probability of non-defaulters $p_2 = \\frac{3}{5}$\n",
        "\n",
        "Therefore, the entropy of the table $S$ before the split is given by\n",
        "\n",
        "\\begin{align}\n",
        "\\text{Entropy}(S_b) &= - (p_1 \\times \\log_{2}(p_1) + p_2 \\times \\log_{2}(p_2))\n",
        "\\end{align}\n",
        "\n",
        "On substituting $p_1 = \\frac{2}{5}$ and $p_2 = \\frac{3}{5}$ in the above equation, we get\n",
        "\n",
        "\\begin{align}\n",
        " &= - \\left( \\left( \\frac{2}{5}\\right) \\times \\log_{2} \\left( \\frac{2}{5}\\right) + \\left( \\frac{3}{5}\\right) \\times \\log_{2}\\left(\\frac{3}{5}\\right) \\right) \\\\\n",
        " &= 0.97\n",
        "\\end{align}\n",
        "\n",
        "Hence, the entropy of the table (considered for IG calculation) before the split is $0.971$\n",
        "\n",
        "You can verify this value by calculating it through Python code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lA6ftFO_J4zy",
        "outputId": "2292b6ee-62b8-4799-f187-9927d6f45003"
      },
      "source": [
        "# Calculate the entropy value of the table (considered for IG calculation) before the split.\n",
        "import numpy as np\n",
        "\n",
        "-((2/5) * np.log2(2/5) + (3/5) * np.log2(3/5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9709505944546686"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JAkD7-8J5hi"
      },
      "source": [
        "**Splitting Root Node wrt Gender**\n",
        "\n",
        "Let us split the root node wrt the **Gender** column.\n",
        "\n",
        "There are 3 instances of `Male` in the table. Out of them, one is a defaulter and 2 are non-defaulters.\n",
        "\n",
        "Let $p_1$ and $p_2$ be the probabilities of `Male` defaulter(s) and `Male` non-defaulter(s) respectively.\n",
        "\n",
        "- So, the probability of `Male` defaulters $p_1 = \\frac{1}{3}$\n",
        "\n",
        "- And, the probability of `Male` non-defaulters $p_2 = \\frac{2}{3}$\n",
        "\n",
        "Male | Defaulter | Non-defaulter\n",
        "--- | --- | --- |\n",
        "Count | $1$ | $2$ |\n",
        "$p_i$ | $\\frac{1}{3}$ | $\\frac{2}{3}$ |\n",
        "$$p_i \\times \\log_2 (p_{i})$$ | $$\\frac{1}{3} \\times \\log_2 \\left(\\frac{1}{3}\\right) = -0.53$$| $$\\frac{2}{3} \\times \\log_2 \\left(\\frac{2}{3}\\right) = -0.39$$ |\n",
        "\n",
        "You can verify these values by calculating them through a Python code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4H-1Zzm3PXRF",
        "outputId": "b174895e-65e7-44d6-cfd4-68e0655a45df"
      },
      "source": [
        "#Calculate the p_i * log_2 (p_i) value(s)\n",
        "print((1/3) * np.log2(1/3))\n",
        "print((2/3) * np.log2(2/3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.5283208335737187\n",
            "-0.38997500048077083\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MHxKRJuPX6N"
      },
      "source": [
        "Therefore, the entropy value wrt to the `Male` instances is\n",
        "\n",
        "\\begin{align}\n",
        "\\text{Entropy} &= - (p_{1} \\times \\log_{2}(p_{1}) + p_{2}\\times \\log_{2}(p_{2})) \\\\\n",
        "&= - (- 0.53  - 0.39) \\\\\n",
        "&= 0.92\n",
        "\\end{align}\n",
        "\n",
        "There are 2 instances of `Female` in the table. Out of them, one is a defaulter another is a non-defaulter.\n",
        "\n",
        "Let $p_1$ and $p_2$ be the probabilities of `Female` defaulter(s) and `Female` non-defaulter(s) respectively.\n",
        "\n",
        "- So, the probability of `Female` defaulters $p_1 = \\frac{1}{2}$\n",
        "\n",
        "- And, the probability of `Female` non-defaulter $p_2 = \\frac{1}{2}$\n",
        "\n",
        "Female | Defaulter | Non-defaulter\n",
        "--- | --- | --- |\n",
        "Count | $1$ | $1$ |\n",
        "$p_{i}$ | $0.5$ | $0.5$ |\n",
        "$$p_i \\times \\log_2 (p_{i})$$ | $$0.5 \\times \\log_2 (0.5) = -0.5$$| $$0.5 \\times \\log_2 (0.5) = -0.5$$ |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZt3J0F_SSlP",
        "outputId": "30cded57-e96e-433a-fa5e-a538222b6c1c"
      },
      "source": [
        "# Calculate the p_i * log_2 (p_i) value(s)\n",
        "0.5 * np.log2(0.5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri_WsEenSRVG"
      },
      "source": [
        "Therefore, the entropy value for the `Female` instances is\n",
        "\n",
        "\\begin{align}\n",
        "\\text{Entropy} &= - (p_1 \\times \\log_{2}(p_1) + p_2 \\times \\log_{2}(p_2)) \\\\\n",
        "&= - (- 0.5 - 0.5) \\\\\n",
        "&= 1\n",
        "\\end{align}\n",
        "\n",
        "Now, let's calculate the *entropy* value after the split that *is the weighted average of the entropy values* for `Male` and `Female` instances. Its calculation process is the same as weighted gini index calculation, i.e.,\n",
        "\n",
        "$$\\text{WE} = (\\text{proportion of Male instances in the Gender column}) \\times (\\text{Entropy of Male}) + (\\text{proportion of Female instances in the Gender column}) \\times (\\text{Entropy of Female})$$\n",
        "\n",
        "- The proportion of `Male` instances in the Gender column = $\\frac{3}{5}$\n",
        "- The proportion of `Female` instances in the Gender column = $\\frac{2}{5}$\n",
        "\n",
        "\\begin{align}\n",
        "\\therefore \\text{Entropy}(S_a) = \\left(\\frac{3}{5}\\right)\\times 0.92 + \\left(\\frac{2}{5}\\right)\\times 1 = 0.95\n",
        "\\end{align}\n",
        "\n",
        "Therefore, the **Information Gain (IG)** obtained by splitting the root node wrt the Gender column is:\n",
        "\n",
        "\\begin{align}\n",
        "\\text{IG} &= \\text{Entropy}(S_{b}) - \\text{Entropy}(S_a) \\\\\n",
        "&= 0.97 - 0.95 \\\\\n",
        "&= 0.02\n",
        "\\end{align}\n",
        "\n",
        "Hence, the information gain is $0.02$ (almost nil) if we use the root node as Gender and perform a split.\n",
        "\n",
        "**Splitting Root Node wrt Education**\n",
        "\n",
        "Let us split the root node wrt the **Education** column.\n",
        "\n",
        "There is 1 instance of `Unknown` in the table and it is a defaulter.\n",
        "\n",
        "Let $p_1$ and $p_2$ be the probabilities of `Unknown` defaulter(s) and `Unknown` non-defaulter(s) respectively.\n",
        "\n",
        "- So, the probability of `Unknown` defaulters $p_1 = \\frac{1}{1} = 1$\n",
        "\n",
        "- And, the probability of `Unknown` non-defaulter $p_2 = \\frac{0}{1} = 0$\n",
        "\n",
        "Unknown | Defaulter | Non-defaulter\n",
        "--- | --- | --- |\n",
        "Count | $1$ | $0$ |\n",
        "$p_i$ | $1$ | $0$ |\n",
        "$$p_i \\times \\log(p_i)$$ | $$1 \\times \\log_2 (1) = 0$$ | $$0 \\times \\log_2 (0) = 0$$ |\n",
        "\n",
        "Hence, the entropy value for the `Unknown` instances will be $0$.\n",
        "\n",
        "There are 2 instances of `Graduate` in the table. One of them is a defaulter and another is a non-defaulter.\n",
        "\n",
        "Let $p_1$ and $p_2$ be the probabilities of `Graduate` defaulter(s) and `Graduate` non-defaulter(s) respectively.\n",
        "\n",
        "- So, the probability of `Graduate` defaulters $p_1 = \\frac{1}{2}$\n",
        "\n",
        "- And, the probability of `Graduate` non-defaulter $p_2 = \\frac{1}{2}$\n",
        "\n",
        "Graduate | Defaulter | Non-defaulter\n",
        "--- | --- | --- |\n",
        "Count | $1$ | $1$ |\n",
        "$p_{i}$ | $0.5$ | $0.5$ |\n",
        "$$p_i \\times \\log_2 (p_{i})$$ | $$0.5 \\times \\log_2 (0.5) = -0.5$$ | $$0.5 \\times \\log_2 (0.5) = -0.5$$ |\n",
        "\n",
        "Therefore, the entropy value for `Graduate` instances is\n",
        "\n",
        "\\begin{align}\n",
        "\\text{Entropy} &= - (p_1 \\times \\log_{2}(p_1) + p_2 \\times \\log_{2}(p_2)) \\\\\n",
        "&= - (- 0.5 - 0.5) \\\\\n",
        "&= 1\n",
        "\\end{align}\n",
        "\n",
        "There are 2 instances of `High School` in the table. Both of them are non-defaulters.\n",
        "\n",
        "Let $p_1$ and $p_2$ be the probabilities of `High School` defaulter(s) and `High School` non-defaulter(s) respectively.\n",
        "\n",
        "- So, the probability of `High School` defaulters $p_1 = \\frac{0}{2} = 0$\n",
        "\n",
        "- And, the probability of `High School` non-defaulter $p_2 = \\frac{2}{2} = 1$\n",
        "\n",
        "High School | Defaulter | Non-defaulter\n",
        "--- | --- | --- |\n",
        "Count | $0$ | $2$ |\n",
        "$p_{i}$ | $0$ | $1$ |\n",
        "$$p_i \\times \\log_2 (p_{i})$$ | $$0 \\times \\log_2 (0) = 0$$| $$1 \\times \\log_2 (1) = 0$$ |\n",
        "\n",
        "Therefore, the entropy value for `High School` instances is $0$\n",
        "\n",
        "Hence, Entropy after split wrt the Education column is:\n",
        "\\begin{align}\n",
        "\\text{Entropy}(S_a) &=  \\left(\\frac{1}{5}\\right)\\times 0 + \\left(\\frac{2}{5}\\right)\\times 1 + \\left(\\frac{2}{5}\\right)\\times 0 \\\\\n",
        "&= 0.4\n",
        "\\end{align}\n",
        "\n",
        "Information Gain obtained splitting the root node wrt the Education columns is\n",
        "\\begin{align}\n",
        "\\text{IG} &= \\text{Entropy}(S_b) - \\text{Entropy}(S_a) \\\\\n",
        "&= 0.97 - 0.4 \\\\\n",
        "&= 0.57\n",
        "\\end{align}\n",
        "\n",
        "Hence, the information gain is $0.57$ if we use the root node as Education and perform a split.\n",
        "\n",
        "*Since the information gain (IG) obtained from the Education column split ($0.57$) is greater than the IG obtained from the Gender column split ($0.02$), we choose the Education column as our Root Node.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiNqQRp32f86"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjOJKh6F9ao8"
      },
      "source": [
        "#### Activity 2: Decision Tree Creation\n",
        "\n",
        "Let us again revisit the original dataset, take a small subset of the dataset, and manually build a decision tree without using the `sklearn` module.\n",
        "\n",
        "As we saw in the correlation matrix, defaulter status depends on few features compared to others. Let us create a smaller dataset by having only the following features to the new dataset:\n",
        "\n",
        "`['EDUCATION', 'SEX', 'PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'DEFAULT']`\n",
        "\n",
        "Also, out of 30,000 samples (or rows), let's take only 24 samples randomly from the original data frame. To do this, use the `sample()` function of the `pandas` module. Its **syntax** is:\n",
        "\n",
        "> `data_frame.sample(n, random_state)`\n",
        "\n",
        "where `n` is the number of samples to be taken and `random_state` allows you to take the same sample randomly when set to an integer value."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the modules, read the dataset and create a Pandas data frame.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cc_client_csv = '/content/UCI_Credit_Card.csv'\n",
        "df = pd.read_csv(cc_client_csv)\n",
        "\n",
        "# Print the first five records\n",
        "print(df.head(), \"\\n\" + \"-\" * 100)\n",
        "\n",
        "# Get the total number of rows and columns, data types of columns and missing values (if exist) in the dataset.\n",
        "print(df.info(), \"\\n\" + \"-\" * 100)\n",
        "\n",
        "# Rename 'PAY_0' to 'PAY_1', and 'default.payment.next.month' to 'DEFAULT'.\n",
        "df.rename(columns = {\"PAY_0\": \"PAY_1\"}, inplace = True)\n",
        "df.rename(columns = {\"default.payment.next.month\": \"DEFAULT\"}, inplace = True)\n",
        "\n",
        "# Remove redundancy in the 'EDUCATION' column.\n",
        "df.loc[df['EDUCATION'] == 0, 'EDUCATION'] = 5\n",
        "df.loc[df['EDUCATION'] == 6, 'EDUCATION'] = 5\n",
        "\n",
        "# Remove redundancy in the 'EDUCATION' column.\n",
        "df.loc[df['MARRIAGE'] == 0, 'MARRIAGE'] = 3"
      ],
      "metadata": {
        "id": "UqVcIlTIr0X5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9093d6d4-47a0-4d91-bf34-9a6984ef5ea7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
            "0   1    20000.0    2          2         1   24      2      2     -1     -1   \n",
            "1   2   120000.0    2          2         2   26     -1      2      0      0   \n",
            "2   3    90000.0    2          2         2   34      0      0      0      0   \n",
            "3   4    50000.0    2          2         1   37      0      0      0      0   \n",
            "4   5    50000.0    1          2         1   57     -1      0     -1      0   \n",
            "\n",
            "   ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
            "0  ...        0.0        0.0        0.0       0.0     689.0       0.0   \n",
            "1  ...     3272.0     3455.0     3261.0       0.0    1000.0    1000.0   \n",
            "2  ...    14331.0    14948.0    15549.0    1518.0    1500.0    1000.0   \n",
            "3  ...    28314.0    28959.0    29547.0    2000.0    2019.0    1200.0   \n",
            "4  ...    20940.0    19146.0    19131.0    2000.0   36681.0   10000.0   \n",
            "\n",
            "   PAY_AMT4  PAY_AMT5  PAY_AMT6  default.payment.next.month  \n",
            "0       0.0       0.0       0.0                           1  \n",
            "1    1000.0       0.0    2000.0                           1  \n",
            "2    1000.0    1000.0    5000.0                           0  \n",
            "3    1100.0    1069.0    1000.0                           0  \n",
            "4    9000.0     689.0     679.0                           0  \n",
            "\n",
            "[5 rows x 25 columns] \n",
            "----------------------------------------------------------------------------------------------------\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 30000 entries, 0 to 29999\n",
            "Data columns (total 25 columns):\n",
            " #   Column                      Non-Null Count  Dtype  \n",
            "---  ------                      --------------  -----  \n",
            " 0   ID                          30000 non-null  int64  \n",
            " 1   LIMIT_BAL                   30000 non-null  float64\n",
            " 2   SEX                         30000 non-null  int64  \n",
            " 3   EDUCATION                   30000 non-null  int64  \n",
            " 4   MARRIAGE                    30000 non-null  int64  \n",
            " 5   AGE                         30000 non-null  int64  \n",
            " 6   PAY_0                       30000 non-null  int64  \n",
            " 7   PAY_2                       30000 non-null  int64  \n",
            " 8   PAY_3                       30000 non-null  int64  \n",
            " 9   PAY_4                       30000 non-null  int64  \n",
            " 10  PAY_5                       30000 non-null  int64  \n",
            " 11  PAY_6                       30000 non-null  int64  \n",
            " 12  BILL_AMT1                   30000 non-null  float64\n",
            " 13  BILL_AMT2                   30000 non-null  float64\n",
            " 14  BILL_AMT3                   30000 non-null  float64\n",
            " 15  BILL_AMT4                   30000 non-null  float64\n",
            " 16  BILL_AMT5                   30000 non-null  float64\n",
            " 17  BILL_AMT6                   30000 non-null  float64\n",
            " 18  PAY_AMT1                    30000 non-null  float64\n",
            " 19  PAY_AMT2                    30000 non-null  float64\n",
            " 20  PAY_AMT3                    30000 non-null  float64\n",
            " 21  PAY_AMT4                    30000 non-null  float64\n",
            " 22  PAY_AMT5                    30000 non-null  float64\n",
            " 23  PAY_AMT6                    30000 non-null  float64\n",
            " 24  default.payment.next.month  30000 non-null  int64  \n",
            "dtypes: float64(13), int64(12)\n",
            "memory usage: 5.7 MB\n",
            "None \n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhsGBdG3BDan",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "outputId": "d126419c-861e-4b2c-f669-ee9cfeb555c9"
      },
      "source": [
        "#Make a new smaller data frame from the original data frame.\n",
        "new_df = df[['EDUCATION', 'SEX', 'PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'DEFAULT']].sample(n = 24, random_state = 5)\n",
        "\n",
        "print(f\"Number of rows = {new_df.shape[0]}\\nNumber of cols = {new_df.shape[1]}\\n\")\n",
        "\n",
        "new_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows = 24\n",
            "Number of cols = 9\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       EDUCATION  SEX  PAY_1  PAY_2  PAY_3  PAY_4  PAY_5  PAY_6  DEFAULT\n",
              "8033           2    1      0      0     -1      0      0     -1        0\n",
              "29952          2    1     -1     -1     -1     -1      0     -1        0\n",
              "2736           2    2      0      0      0      0      0      0        0\n",
              "29677          2    1      0      0     -2     -2     -1      0        0\n",
              "3285           1    1      0      0      0      0      2      0        0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7fa4c6a9-c041-4228-a5cd-7a1652078f6a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>EDUCATION</th>\n",
              "      <th>SEX</th>\n",
              "      <th>PAY_1</th>\n",
              "      <th>PAY_2</th>\n",
              "      <th>PAY_3</th>\n",
              "      <th>PAY_4</th>\n",
              "      <th>PAY_5</th>\n",
              "      <th>PAY_6</th>\n",
              "      <th>DEFAULT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>8033</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29952</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2736</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29677</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-2</td>\n",
              "      <td>-2</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3285</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7fa4c6a9-c041-4228-a5cd-7a1652078f6a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7fa4c6a9-c041-4228-a5cd-7a1652078f6a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7fa4c6a9-c041-4228-a5cd-7a1652078f6a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f1c43a7e-cb26-4920-b8e8-43a66b4fd902\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f1c43a7e-cb26-4920-b8e8-43a66b4fd902')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f1c43a7e-cb26-4920-b8e8-43a66b4fd902 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "new_df",
              "summary": "{\n  \"name\": \"new_df\",\n  \"rows\": 24,\n  \"fields\": [\n    {\n      \"column\": \"EDUCATION\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2,\n          1,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SEX\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PAY_1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": -2,\n        \"max\": 3,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          0,\n          -1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PAY_2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": -2,\n        \"max\": 2,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          -1,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PAY_3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": -2,\n        \"max\": 7,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0,\n          7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PAY_4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": -2,\n        \"max\": 7,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          -1,\n          7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PAY_5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": -2,\n        \"max\": 7,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          -1,\n          7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PAY_6\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": -2,\n        \"max\": 7,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0,\n          7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"DEFAULT\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w96tVXUSCGwF"
      },
      "source": [
        "**Algorithmic Approach**\n",
        "\n",
        "Since we are not using the `sklearn` module to build a decision tree, we have to do the following to do the same manually:\n",
        "\n",
        "1. Compute entropy for a dataset using the following result.\n",
        "\n",
        "\\begin{align}\n",
        "\\text{Entropy} = -\\sum_{i=1}^{N}p_i \\times \\log_{2}(p_i)\n",
        "\\end{align}\n",
        "\n",
        "2. For each attribute/feature:\n",
        "  - calculate entropy for all categorical values\n",
        "  - calculate the weighted average of the entropy values for the current attribute\n",
        "  - calculate information gain (IG) for the current attribute\n",
        "\n",
        "3. Choose the feature/attribute having the greatest IG value to split the root node.\n",
        "\n",
        "4. Repeat the splitting process until we get pure nodes (all leaf nodes).\n",
        "\n",
        "Now let's define a Python function, say `compute_entropy()`, that takes a data frame as an input and returns the entropy value as an output. Inside the function:\n",
        "\n",
        "- Get the target variable values from the new small data frame.\n",
        "\n",
        "- Declare a new variable, say `entropy` and set its value equal to 0.\n",
        "\n",
        "- Determine the unique values of the target variable. In our case, 0 and 1.\n",
        "\n",
        "- For all the unique values, calculate entropy using a `for` loop. Inside the `for` loop:\n",
        "\n",
        "  - Calculate the probability of an event, say `i` as per the entropy formula\n",
        "  \n",
        "  - Calculate the entropy values\n",
        "\n",
        "- Finally, return the value of entropy."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_col_name = new_df.columns[-1]\n",
        "target_col_name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nKg6ETWE7485",
        "outputId": "d641a946-657c-4354-fa97-7feb2daa954d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'DEFAULT'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_values = new_df[target_col_name].unique()\n",
        "unique_values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtKOIlPA8Az-",
        "outputId": "5824580f-ab21-4477-f767-b15942276fbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_df[target_col_name].value_counts()[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIVIbSrn8OJk",
        "outputId": "9dc40dae-3dbe-4a3a-c24e-13f168514a17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_df[target_col_name].value_counts()[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yowwWclN8R30",
        "outputId": "497c61f5-5531-49f6-cc31-a4df9a845f2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(new_df[target_col_name])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Yogeiee8YeE",
        "outputId": "298e8168-726d-4b6d-fd46-2064f2ac8ae7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OILGTZsdCFcx"
      },
      "source": [
        "#  Create a Python function that takes a data frame as an input and returns entropy value as an output.\n",
        "def entropy_before_split(new_df):\n",
        "    target_col_name = new_df.columns[-1]\n",
        "    entropy = 0\n",
        "    unique_values = new_df[target_col_name].unique()\n",
        "    for value in unique_values:\n",
        "        probability_i = new_df[target_col_name].value_counts()[value] / len(new_df[target_col_name])\n",
        "        entropy += -probability_i * np.log2(probability_i)\n",
        "    return entropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgtNF5WUChUX"
      },
      "source": [
        "In the above code:\n",
        "\n",
        "- `new_df.columns[-1]` will return the name of the target column.\n",
        "\n",
        "- `new_df[target_col_name].unique()` will return an array containing unique values\n",
        "\n",
        "- `new_df[target_col_name].value_counts()` will return a series containing the count of all the unique values contained in the `new_df[target_col_name]` series.\n",
        "\n",
        "- `new_df[target_col_name].value_counts()[value]` will return the count at index `value` in the `new_df[target_col_name].value_counts()` series.\n",
        "  \n",
        "  Eg., `new_df['DEFAULT'].value_counts()` returns the following series:\n",
        "\n",
        "  ```\n",
        "  0    19\n",
        "  1     5\n",
        "  Name: DEFAULT, dtype: int64\n",
        "  ```\n",
        "\n",
        "  Therefore:\n",
        "  - If `value = 0`, then `new_df['DEFAULT'].value_counts()[value]` will return `19`\n",
        "\n",
        "  - Similarly, if `value = 1`, then `new_df['DEFAULT'].value_counts()[value]` will return `5`\n",
        "\n",
        "- `entropy += -probability_i * np.log2(probability_i)` will calculate probabilities for `value = 0` and `value = 1`, multiply with their corresponding logarithm (at base 2) values and will add them to get the **entropy before split** value.\n",
        "\n",
        "  \\begin{align}\n",
        "  \\text{Entropy} = -\\sum_{i=1}^{N}p_i \\times \\log_{2}(p_i)\n",
        "  \\end{align}\n",
        "\n",
        "\n",
        "The next step is to compute **entropy after** the **split**, i.e., the entropy of a feature variable (or weighted entropy of the unique values of a feature) in the `new_df` data frame after splitting the root node.\n",
        "\n",
        "For this, we need to define another function, say `entropy_after_split()`. It should take `new_df` and a feature of `new_df` as inputs and return the weighted entropy for a feature after the split. Inside the function:\n",
        "\n",
        "- Get the target variable from a data frame.\n",
        "\n",
        "- Determine the unique values of each feature (`'PAY_1'`, `'PAY_2'` etc.)\n",
        "\n",
        "- Declare a new variable, say `weighted_entropy` and set it equal to 0 initially. It will eventually store the weighted entropy value for a feature.\n",
        "\n",
        "- Loop through all the unique values of a feature using a `for` loop. Inside the loop:\n",
        "\n",
        "  - Define a new variable, say `entropy_of_feat_uniq_val` and set it equal to 0 initially. It will eventually store the entropy of a unique value of a feature variable.\n",
        "\n",
        "  - Loop through the unique values of the target variable that are available for a feature variable. Inside the inner `for` loop.\n",
        "\n",
        "    - Calculate the probability of a unique value of a feature variable\n",
        "\n",
        "    - Calculate the corresponding entropy by multiplying the probability value (obtained in the above step) with its logarithm (at base 2) value, i.e., $-p_i \\times \\log_{2}(p_i)$\n",
        "\n",
        "  - Exit the inner `for` loop and inside the outer `for` loop, calculate the proportion of a unique value in the feature.\n",
        "\n",
        "  - Calculate the weighted entropy value of a feature by computing the weighted average of the individual entropy values of each unique value.\n",
        "\n",
        "- Return the weighted entropy value.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azZK0x_vkKzY"
      },
      "source": [
        "#  Create a function to calculate the entropy after split the value for a feature.\n",
        "def entropy_after_split(new_df, feature):\n",
        "  target_col_name = new_df.columns[-1]\n",
        "  feature_unique_values = new_df[feature].unique()\n",
        "  weighted_entropy = 0\n",
        "  for feat_uniq_val in feature_unique_values:\n",
        "      entropy_of_feat_uniq_val = 0 # To store the entropy value of a unique value of a feature variable.\n",
        "      corres_avail_target_val = new_df.loc[new_df[feature] == feat_uniq_val, target_col_name].value_counts().index.values\n",
        "      for val in corres_avail_target_val:\n",
        "          prob_val = new_df[new_df[feature] == feat_uniq_val][target_col_name].value_counts()[val] / new_df.shape[0]\n",
        "          entropy_of_feat_uniq_val += - prob_val * np.log2(prob_val)\n",
        "      prop_of_feat_uniq_val = new_df[new_df[feature] == feat_uniq_val].shape[0] / new_df.shape[0]\n",
        "      weighted_entropy += prop_of_feat_uniq_val * entropy_of_feat_uniq_val\n",
        "  return weighted_entropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eennmDlH4ZzH"
      },
      "source": [
        "In the above code:\n",
        "\n",
        "- `new_df.columns[-1]` returns the name of the target column.\n",
        "\n",
        "- `new_df[feature].unique()` returns the unique values in a feature column.\n",
        "\n",
        "- `new_df.loc[new_df[feature] == feat_uniq_val, target_col_name].value_counts().index.values` returns the unique values that are available for a feature. Eg., for `new_df[EDUCATION] == 1`, corresponding `DEFAULT` values are `0` only. This means in the `new_df` data frame there is NO `Graduate` defaulter. Hence, the available `DEFAULT` values for `new_df[EDUCATION] == 1` is only `0`. So the inner `for` loop should iterate only once.\n",
        "\n",
        "- `new_df[new_df[feature] == feat_uniq_val]` will return a data frame containing only the `feat_uniq_val` in the `feature` column.\n",
        "\n",
        "  Eg., if `feature` is `EDUCATION` and `feat_unique_val` is `3`, then `new_df[new_df[feature] == feat_uniq_val]` will return\n",
        "\n",
        "  ```\n",
        "        EDUCATION\tSEX\tPAY_1\tPAY_2\tPAY_3\tPAY_4\tPAY_5\tPAY_6\tDEFAULT\n",
        "  28568\t    3\t  2\t    0\t    0\t    0\t   -2\t   -2\t   -2\t      0\n",
        "  29524\t    3\t  1\t    1\t    2\t    2\t    2\t    2\t    2\t      1\n",
        "  ```\n",
        "\n",
        "- `new_df[new_df[feature] == feat_uniq_val][target_col_name].value_counts()`will return a series containing the count of all the unique values contained in the target column in the `new_df[new_df[feature] == feat_uniq_val]` data frame.\n",
        "\n",
        "  Eg., if `feature` is `EDUCATION` and `feat_unique_val` is `3`, then `new_df[new_df[feature] == feat_uniq_val]['DEFAULT].value_counts()` will return\n",
        "\n",
        "  ```\n",
        "  0    1\n",
        "  1    1\n",
        "  Name: DEFAULT, dtype: int64\n",
        "  ```\n",
        "\n",
        "- `new_df[new_df[feature] == feat_uniq_val][target_col_name].value_counts()[val]` will return the count at index `val` in the `new_df[new_df[feature] == feat_uniq_val][target_col_name].value_counts()` series.\n",
        "  \n",
        "  Eg., if `feature` is `EDUCATION` and `feat_unique_val` is `3`, then `new_df[new_df[feature] == feat_uniq_val]['DEFAULT].value_counts()` will return\n",
        "\n",
        "  ```\n",
        "  0    1\n",
        "  1    1\n",
        "  Name: DEFAULT, dtype: int64\n",
        "  ```\n",
        "\n",
        "  Therefore:\n",
        "  - If `val = 0`, then `new_df[new_df[feature] == feat_uniq_val][target_col_name].value_counts()[val]` will return `1`\n",
        "\n",
        "  - Similarly, if `value = 1`, then `new_df[new_df[feature] == feat_uniq_val][target_col_name].value_counts()[val]` will return `1`.\n",
        "\n",
        "- `new_df[new_df[feature] == feat_uniq_val][target_col_name].value_counts()[val] / new_df.shape[0]` will return the probability values for `val`.\n",
        "\n",
        "- `entropy_of_feat_uniq_val += - prob_val * np.log2(prob_val)` will calculate the individual entropies of each unique value in the target variable for a unique value in a feature variable.\n",
        "\n",
        "- `new_df[new_df[feature] == feat_uniq_val].shape[0] / new_df.shape[0]` will calculate the proportion of a unique value in a feature.\n",
        "\n",
        "- `weighted_entropy += prop_of_feat_uniq_val * entropy_of_feat_uniq_val` will calculate the weighted entropy of a feature.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "Now let's define a function to calculate the information gain (IG) to split a root node. Let's call it `root_node_attribute()` function. This function takes a data frame as an input and returns the node having the greatest information gain value. Inside the function:\n",
        "\n",
        "- Create empty lists to store entropy and information gain for respective nodes\n",
        "\n",
        "- Use `for` loop to obtain entropy and information gain for all the features set in data frame. Inside for loop:\n",
        "\n",
        "  - Get entropy and information gain only for feature columns\n",
        "\n",
        "  - Find the entropy after split wrt features and append to `entropies_list` using the `append()` method\n",
        "\n",
        "  - Obtain the information gain using the formula $\\text{IG} = E_b - E_a$ wrt features and append to the `info_gain_list` using the `append()` method\n",
        "\n",
        "- Finally, return the feature having the greatest information gain value.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTCMGLpEx18W"
      },
      "source": [
        "#  Create a function to calculate the Information Gain for split operation.\n",
        "def root_node_attribute(new_df):\n",
        "    entropies_list = []\n",
        "    info_gain_list = []\n",
        "    for feature in new_df.columns[:-1]:\n",
        "        entropies_list.append(entropy_after_split(new_df, feature))\n",
        "        info_gain_list.append(entropy_before_split(new_df) - entropy_after_split(new_df, feature))\n",
        "    return new_df.columns[:-1][np.argmax(info_gain_list)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7SwmzNiyJ2n"
      },
      "source": [
        "In the above code:\n",
        "\n",
        "- The `entropies_list` and `info_gain_list` represent the entropy and information gain list respectively.\n",
        "\n",
        "- Within for loop:  \n",
        "\n",
        "  - `entropy_after_split(new_df, feature)` function will return entropy after split with respect to various `feature`\n",
        "\n",
        "  - The `append()` method will append the $E_a$ values returned by the `entropy_after_split()` function to `entropies_list`\n",
        "\n",
        "  - `entropy_before_split(new_df) - entropy_after_split(new_df, feature)` will return information gain wrt corresponding `feature`\n",
        "\n",
        "  - the `append()` method will append the $IG$ values to `info_gain_list` using the `append()` method\n",
        "\n",
        "- `np.argmax(info_gain_list)` will return the list index which exhibit maximum value.\n",
        "\n",
        "  In general, `np.argmax()` function returns the index of the algebraically greatest item in an array or list.\n",
        "  \n",
        "  Eg., for the list `[-2, 4, 7, 12, 1, .34]`, the `np.argmax()` function will return 3 because the list contains the greatest item (12) at index 3,.\n",
        "\n",
        "- `new_df.columns[:-1][np.argmax(info_gain_list)]` will return the feature name corresponding to the list index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "yzivYz53yN0o",
        "outputId": "b2db8f81-f21f-4551-963a-f8aefb604d0c"
      },
      "source": [
        "# Find out the attribute (or feature) to split the root node.\n",
        "root_node_att = root_node_attribute(new_df)\n",
        "root_node_att"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'PAY_1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzaxw9wOySFa"
      },
      "source": [
        "What happens when use the `root_node_attribute()` function:\n",
        "\n",
        "- `entropies_list.append(entropy_after_split(new_df, feature))` will return a list of entropies wrt to each node. Eg.:\n",
        "\n",
        "  ```\n",
        "  entopies_list =[0.730, 0.864, 0.576, 0.604, 0.586, 0.671, 0.634, 0.692]\n",
        "  index:          0      1      2      3      4      5      6      7\n",
        "  ```\n",
        "- `info_gain_list.append(entropy_before_split(new_df) - entropy_after_split(new_df, feature))` will return a list of information gain wrt the `entropies_list`. Eg:\n",
        "\n",
        "  ```\n",
        "  info_gain_list = [0.0075, -0.1259, 0.1617, 0.1337, 0.1516, 0.0667, 0.1036, 0.0458]\n",
        "  index:             0       1        2       3       4       5       6       7\n",
        "  ```\n",
        "\n",
        "- The lowest entropy is exhibited by `index = 2` element with a value of `0.576`\n",
        "\n",
        "- Consequently, `index = 2` exhibits the highest information gain of `0.16176021330557655` which implies `index = 2` indicates the `PAY_1` feature based on the feature returned by `root_node_attribute()` function.\n",
        "\n",
        "\n"
      ]
    }
  ]
}