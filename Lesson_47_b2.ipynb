{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poudyaldiksha/Data-Science-project/blob/main/Lesson_47_b2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPBYmVldndM4"
      },
      "source": [
        "# Lesson 47: Logistic Regression - Heart Disease Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoDdcwUtJ07u"
      },
      "source": [
        "\n",
        "\n",
        "In the previous few classes, you learnt how a logistic regression model classifies labels behind the scenes.\n",
        "\n",
        "In this class, we will continue to build a multivariate logistic regression model to predict whether a patient has heart disease."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAQyK7yfO14I"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrTUGKPJO61D"
      },
      "source": [
        "#### Recap\n",
        "\n",
        "Run the code below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmRB05lddS--",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33fd3257-f78a-401a-9a5e-541c5a02bf49"
      },
      "source": [
        "# Import the required modules and load the heart disease dataset. Also, display the first five rows.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "csv_file = '/content/heart.csv'\n",
        "df = pd.read_csv(csv_file)\n",
        "print(\"\\n\", df.head(), \"\\n\", df.info(), \"\\n\")\n",
        "\n",
        "# Print the number of records with and without heart disease\n",
        "print(\"Number of records in each label are\")\n",
        "print(df['target'].value_counts())\n",
        "\n",
        "# Print the percentage of each label\n",
        "print(\"\\nPercentage of records in each label are\")\n",
        "print(df['target'].value_counts() * 100 / df.shape[0])\n",
        "\n",
        "# Split the training and testing data\n",
        "X = df.drop(columns = 'target')\n",
        "y = df['target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 303 entries, 0 to 302\n",
            "Data columns (total 14 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   age       303 non-null    int64  \n",
            " 1   sex       303 non-null    int64  \n",
            " 2   cp        303 non-null    int64  \n",
            " 3   trestbps  303 non-null    int64  \n",
            " 4   chol      303 non-null    int64  \n",
            " 5   fbs       303 non-null    int64  \n",
            " 6   restecg   303 non-null    int64  \n",
            " 7   thalach   303 non-null    int64  \n",
            " 8   exang     303 non-null    int64  \n",
            " 9   oldpeak   303 non-null    float64\n",
            " 10  slope     303 non-null    int64  \n",
            " 11  ca        303 non-null    int64  \n",
            " 12  thal      303 non-null    int64  \n",
            " 13  target    303 non-null    int64  \n",
            "dtypes: float64(1), int64(13)\n",
            "memory usage: 33.3 KB\n",
            "\n",
            "    age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
            "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
            "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
            "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
            "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
            "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
            "\n",
            "   ca  thal  target  \n",
            "0   0     1       1  \n",
            "1   0     2       1  \n",
            "2   0     2       1  \n",
            "3   0     2       1  \n",
            "4   0     2       1   \n",
            " None \n",
            "\n",
            "Number of records in each label are\n",
            "target\n",
            "1    165\n",
            "0    138\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Percentage of records in each label are\n",
            "target\n",
            "1    54.455446\n",
            "0    45.544554\n",
            "Name: count, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUuB2VRPPuzf"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyTyA2t3PpvR"
      },
      "source": [
        "####Activity 1: Multivariate Logistic Regression\n",
        "\n",
        "Let's include all the features present in the heart disease dataset to build a multivariate logistic regression model using the `sklearn` module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IebsK3oUQQWv"
      },
      "source": [
        "#  Create a multivariate logistic regression model. Also, predict the target values for the train set.\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "lg_clf_1 = LogisticRegression()\n",
        "lg_clf_1.fit(X_train, y_train)\n",
        "lg_clf_1.score(X_train, y_train)\n",
        "\n",
        "# Predict the target values for the train set.\n",
        "y_train_pred = lg_clf_1.predict(X_train)\n",
        "\n",
        "print(f\"{'Train Set'.upper()}\\n{'-' * 75}\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_train, y_train_pred))\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_train, y_train_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "help(lg_clf_1)"
      ],
      "metadata": {
        "id": "goudO1N3aGWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KQW3TOWox4_"
      },
      "source": [
        "#Predict the target values for the test set.\n",
        "y_test_pred = lg_clf_1.predict(X_test)\n",
        "\n",
        "print(f\"{'Test Set'.upper()}\\n{'-' * 75}\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_test_pred))\n",
        "\n",
        "print(\"\\nClassification Report\")\n",
        "print(classification_report(y_test, y_test_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaFmty9RSSJN"
      },
      "source": [
        "As you can see,\n",
        "- The FP and FN values in the confusion matrix are low\n",
        "- The precision and recall values are also good\n",
        "- The f1-score is also greater than **0.7**\n",
        "\n",
        "\n",
        "\n",
        "But this logistic regression model (refer to the object stored in the `lg_clf_1` variable) is created using all the features (or independent variables). It is quite possible that not all features are of imporatance for the classification of the labels in the `target` column. Therefore, we still can improve the model by reducing the number of features to obtain higher f1-scores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_JXv6P_V_vN"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vppY0xJcg_ld"
      },
      "source": [
        "#### Activity 2: Data Standardisation\n",
        "\n",
        "As you must have observed, when the logistic regression is applied we got the following warning message shown below quite a few times:\n",
        "```\n",
        "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
        "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
        "\n",
        "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
        "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
        "Please also refer to the documentation for alternative solver options:\n",
        "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
        "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
        "```\n",
        "\n",
        "\n",
        "One of the reason for the popping-up of the warning message is poorly scaled data. Here are a couple of ways to avoid `ConvergenceWarning` message:\n",
        "\n",
        "1. Increase the number of iterations i.e. set the value of `max_iter` parameter to 100 i.e. `max_iter = 100` in the `LogisticRegression` constructor.\n",
        "\n",
        "2. Scale the data using one of the normalisation methods, say standard normalisation.\n",
        "\n",
        "Therefore, let's create a function `standard_scalar()` to normalise the `X_train` and `X_test` data-frames using standard normalisation method i.e.\n",
        "\n",
        "$$x_{\\text{std}} = \\frac{(x_i - \\mu)}{\\sigma} $$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwYdDr3oiE4U"
      },
      "source": [
        "#Normalise the train and test data-frames using the standard normalisation method.\n",
        "def standard_scaler(series):\n",
        "  new_series = (series - series.mean()) / series.std()\n",
        "  return new_series\n",
        "\n",
        "norm_X_train = X_train.apply(standard_scaler, axis = 0)\n",
        "norm_X_test = X_test.apply(standard_scaler, axis = 0)\n",
        "\n",
        "norm_X_train.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKkQFKBki7KL"
      },
      "source": [
        "# Display descriptive statistics for the normalised values of the features for the test data-frames.\n",
        "norm_X_test.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6Fw8ItUlKQy"
      },
      "source": [
        "As we can observe in the output, the data is normalised because the mean and standard deviation values for each column are 0 and 1 respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8t9xTqji7Fv"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcnUnBLDWBUm"
      },
      "source": [
        "####Activity 3: Features Selection Using RFE\n",
        "\n",
        "Our next task is to select the relevant features from all the features that contribute to a person having a heart disease. The irrelevant features do not help in increasing the accuracy of a prediction model. Secondly, they also increase the training time of a model. You don't want to have either a very few features or too many of them in your prediction model.\n",
        "\n",
        "So, the question is **how to select features?**\n",
        "\n",
        "One simpler way is trial and error. You can pick **any one feature** at a time, build a prediction model and evaluate it.\n",
        "\n",
        "Similarly, you pick **any two features** at a time, a prediction model and evaluate it. For example\n",
        "- 1, 2\n",
        "- 1, 3\n",
        "- 1, 4\n",
        "etc.\n",
        "\n",
        "Similarly, you pick **any three features** at a time, a prediction model and evaluate it. For example\n",
        "- 1, 2, 3\n",
        "- 1, 2, 4\n",
        "- 2, 3, 4\n",
        "etc.\n",
        "\n",
        "And so on. However, all this is a very time-consuming process to do manually. Instead, you can use the `RFE` (Recursive Feature Elimination) class of the `sklearn.feature_selection` module.It is a  backward feature selection technique and is based on **feature importance**.\n",
        "\n",
        "So let's try to find the optimal number of features required using RFE to build a logistic regression model to predict whether a person has heart disease. Here is the list of steps below that we will follow for this purpose:\n",
        "\n",
        "1. Import the following modules\n",
        "```\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "```\n",
        "\n",
        "2. Create an empty dictionary and store it in a variable called `dict_rfe`.\n",
        "\n",
        "3. Create a `for` loop that iterates through all the columns in normalised training data-frame. Inside the loop:\n",
        "   \n",
        "   - Create an object of `LogisticRegression` class and store it in a variable called `lg_clf_2`.\n",
        "   \n",
        "   - Create an object of `RFE` class and store it in a variable called `rfe`. Inside the `RFE()` constructor, pass the object of logistic regression and the number of features to be selected by RFE as inputs.\n",
        "   \n",
        "   - Call the `fit()` function of the `RFE` class to train a logistic regression model on the train set with `i` number of features where `i` goes from `1` to `len(X_train.columns)`.\n",
        "   \n",
        "   - The `support_` attribute holds rank value(s) of the selected feature(s) where rank `1` denotes the most important feature.\n",
        "   \n",
        "   - Create a list to store the important features in a variable called `rfe_features`.\n",
        "   \n",
        "   - Create a new data-frame having the features selected by RFE store it in a variable called `rfe_X_train`.\n",
        "   \n",
        "   - Create another `LogisticRegression` object, store it in a variable called `lg_clf_3` and build a logistic regression model using the `rfe_X_train` data-frame and `y_train` series.\n",
        "   \n",
        "   - Predict the target values for the normalised test set (containing the feature(s) selected by RFE) by calling the `predict()` function on `lg_clf_3` object.\n",
        "   \n",
        "   - Calculate f1-scores using the function `f1_score()` function of `sklearn.metrics` module that returns a NumPy array containing f1-scores for both the classes. Store the array in a variable called `f1_scores_array`. The **syntax** for the `f1_score()` function is `f1_score(y_true, y_pred, average = None)`\n",
        "     where `y_true` and `y_pred` are the actual and predicted labels respectively, and `average = None` parameter returns the scores for each class.\n",
        "\n",
        "   - Add the number of selected features and corresponding features & f1-scores as key-value pairs in the `dict_rfe` dictionary."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(RFE)"
      ],
      "metadata": {
        "id": "TkKR5OPCklRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(norm_X_train.columns)"
      ],
      "metadata": {
        "id": "ZDZ3IcpQgTWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression()\n",
        "rfe_model = RFE(model,n_features_to_select=3)\n",
        "rfe_model.fit(norm_X_train,y_train)\n",
        "print(rfe_model.support_)\n",
        "print(rfe_model.ranking_)\n",
        "print(norm_X_train.columns)\n",
        "features = list(norm_X_train.columns[rfe_model.support_])\n",
        "print(features)"
      ],
      "metadata": {
        "id": "9by31_gMjXP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTJAe6qdSxTu"
      },
      "source": [
        "# Create a dictionary containing the different combination of features selected by RFE and their corresponding f1-scores.\n",
        "# Import the libraries\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Create the empty dictionary.\n",
        "dict_rfe = {}\n",
        "\n",
        "# Create a loop\n",
        "for i in range(1, len(X_train.columns) + 1):\n",
        "  lg_clf_2 = LogisticRegression()\n",
        "  rfe = RFE(lg_clf_2, n_features_to_select=i) # 'i' is the number of features to be selected by RFE to fit a logistic regression model on norm_X_train and y_train.\n",
        "  rfe.fit(norm_X_train, y_train)\n",
        "\n",
        "  rfe_features = list(norm_X_train.columns[rfe.support_]) # A list of important features chosen by RFE.\n",
        "  rfe_X_train = norm_X_train[rfe_features]\n",
        "\n",
        "  # Build a logistic regression model using the features selected by RFE.\n",
        "  lg_clf_3 = LogisticRegression()\n",
        "  lg_clf_3.fit(rfe_X_train, y_train)\n",
        "\n",
        "  # Predicting 'y' values only for the test set as generally, they are predicted quite accurately for the train set.\n",
        "  y_test_pred = lg_clf_3.predict(norm_X_test[rfe_features])\n",
        "\n",
        "  f1_scores_array = f1_score(y_test, y_test_pred, average = None)\n",
        "  dict_rfe[i] = {\"features\": list(rfe_features), \"f1_score\": f1_scores_array} # 'i' is the number of features to be selected by RFE."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7Usuq9UBZwV"
      },
      "source": [
        "In the above code:\n",
        "\n",
        "1. ```\n",
        "   lg_clf_2 = LogisticRegression()\n",
        "   rfe = RFE(lg_clf_2, i)\n",
        "   rfe.fit(norm_X_train, y_train)\n",
        "   ```\n",
        "   part gets the most important features using RFE.\n",
        "\n",
        "2. ```\n",
        "   rfe_features = list(norm_X_train.columns[rfe.support_])\n",
        "   rfe_X_train = norm_X_train[rfe_features]\n",
        "   ```\n",
        "   part creates a new data-frame containing the values of the most important feature(s) selected by RFE.\n",
        "\n",
        "3. ```\n",
        "   lg_clf_3 = LogisticRegression()\n",
        "   lg_clf_3.fit(rfe_X_train, y_train)\n",
        "   ```\n",
        "   part builds a logistic regression model using the most important feature(s) selected by RFE.\n",
        "\n",
        "4. ```\n",
        "   y_test_pred = lg_clf_3.predict(norm_X_test[rfe_features])\n",
        "   ```\n",
        "   part predicts the target values on the test set only as generally a machine learning model performs well on the training set.\n",
        "\n",
        "5. ```\n",
        "   f1_scores_array = f1_score(y_test, y_test_pred, average = None)\n",
        "   ```\n",
        "   part calculates f1-scores\n",
        "\n",
        "6. ```\n",
        "   dict_rfe[i] = {\"features\": list(rfe_features), \"f1_score\": f1_scores_array}\n",
        "   ```\n",
        "   part adds the number of features, features and their corresponding f1-scores as key-value pairs to the dictionary stored in the `dict_rfe` variable.\n",
        "\n",
        "Let's print the dictionary created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx87KQFxSxTx"
      },
      "source": [
        "# Print the dictionary created in the previous exercise.\n",
        "dict_rfe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CV8-ktoQAXTo"
      },
      "source": [
        "Let's convert the `dict_rfe` dictionary to a Pandas DataFrame using the `from_dict()` function of `pandas` module. Pass `orient = index` parameter to the function to orient the DataFrame index-wise. Otherwise, the keys of the dictionary i.e. (1 through 12) will become columns.\n",
        "\n",
        "Moreover, we need columns having larger width in the data-frame as the columns will contain lists and arrays as their values. To do this you can use the `max_colwidth` attribute.\n",
        "\n",
        "**Syntax:** `pd.options.display.max_colwidth = W`\n",
        "\n",
        "where `W` is the required column width.\n",
        "\n",
        "Let's set the column widths to 100.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGKNSmKZaTck"
      },
      "source": [
        "# Convert the dictionary to the dataframe\n",
        "pd.options.display.max_colwidth = 100\n",
        "f1_df = pd.DataFrame.from_dict(dict_rfe, orient = 'index')\n",
        "f1_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAxf5qBWwt4j"
      },
      "source": [
        "From the above data-frame, we can see that we get the best f1-scores for both the classes when we have 3 features which are `cp, oldpeak` and `ca`. Beyond this point, the number of features increase but the f1-scores increase only marginally. Hence, it is best to have these many features to build a prediction model to predict whether a patient has heart disease.\n",
        "\n",
        "Let's now rebuild a logistic regression model with the ideal number of features to predict whether a person has a heart disease."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEftTX0v6DLF"
      },
      "source": [
        "#  Logistic Regression with the ideal number of features.\n",
        "lg_clf_4 = LogisticRegression()\n",
        "rfe = RFE(lg_clf_4, n_features_to_select=3)\n",
        "\n",
        "rfe.fit(norm_X_train, y_train)\n",
        "\n",
        "rfe_features = norm_X_train.columns[rfe.support_]\n",
        "print(rfe_features)\n",
        "final_X_train = norm_X_train[rfe_features]\n",
        "\n",
        "lg_clf_4 = LogisticRegression()\n",
        "lg_clf_4.fit(final_X_train, y_train)\n",
        "\n",
        "y_test_predict = lg_clf_4.predict(norm_X_test[rfe_features])\n",
        "final_f1_scores_array = f1_score(y_test, y_test_predict, average = None)\n",
        "print(final_f1_scores_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----"
      ],
      "metadata": {
        "id": "UzCul5Td8Kkc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elaNnzqZZR9C"
      },
      "source": [
        "###Logistic Regression - Multiclass Classification I\n",
        "\n",
        "So far you have learnt to build a logistic regression model for only two labels. There are a few cases when you have to classify more than two labels. So the classification of such labels is called multiclass classification. In order to practice it, we are going to solve another problem-statement wherein we have to classify different types of glasses based on their chemical and physical composition. Let's call this project glass-type classification.\n",
        "\n",
        "Also, in this class we will learn to create graphs with Plotly.\n",
        "\n",
        "**Dataset Description:**\n",
        "\n",
        "The dataset used in this problem statement involves the classification of samples of different glasses based on their physical and chemical properties. They are as follows:\n",
        "\n",
        "1. **RI:** Refractive Index\n",
        "\n",
        "2. **Na:** Sodium\n",
        "\n",
        "3. **Mg:** Magnesium\n",
        "\n",
        "4. **Al:** Aluminum\n",
        "\n",
        "5. **Si:** Silicon\n",
        "\n",
        "6. **K:** Potassium\n",
        "\n",
        "7. **Ca:** Calcium\n",
        "\n",
        "8. **Ba:** Barium\n",
        "\n",
        "9. **Fe:** Iron\n",
        "\n",
        "The chemical compositions are measured as the weight per cent in their corresponding oxides such as $\\text{Na}_2\\text{O}$, $\\text{Al}_2\\text{O}_3$, $\\text{Si}\\text{O}_2$ etc.\n",
        "\n",
        "There are seven types (classes or labels) of glass listed; they are:\n",
        "\n",
        "* **Class 1:** used for making building windows (float processed)\n",
        "\n",
        "* **Class 2:** used for making building windows (non-float processed)\n",
        "\n",
        "* **Class 3:** used for making vehicle windows (float processed)\n",
        "\n",
        "* **Class 4:** used for making vehicle windows (non-float processed)\n",
        "\n",
        "* **Class 5:** used for making containers\n",
        "\n",
        "* **Class 6:** used for making tableware\n",
        "\n",
        "* **Class 7:** used for making headlamps\n",
        "\n",
        "A float-type glass refers to the process used to make the glass. The molten glass is introduced into a bath of molten tin, causing the glass to float freely. These glasses are used to absorb heat and UV rays.\n",
        "\n",
        "**Dataset Credits:** https://archive.ics.uci.edu/ml/datasets/Glass+Identification\n",
        "\n",
        "\n",
        "**Citation:** Dua, D., & Graff, C.. (2017). UCI Machine Learning Repository"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2AQeeHML2pC"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_eGf00TL3k3"
      },
      "source": [
        "#### Activity 1: Data Loading\n",
        "\n",
        "So let's go through the routine steps before we build a logistic regression model and explore the dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBZRijsRUXhp"
      },
      "source": [
        "# Load the dataset.\n",
        "# Import the necessary libraries.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset.\n",
        "file_path = '/content/glass-types.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pfQ7RO7MhB7"
      },
      "source": [
        "As you can see from the output, the data columns have strange headers (or titles). Let's load the dataset again without the column headers. For this, you can pass a parameter called `header` inside the `read_csv()` function of the `pandas` module and set its value equal to `None`.\n",
        "\n",
        "**Syntax:** `pd.read_csv(file_path, header = None)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uH0Mj4SBMXtp"
      },
      "source": [
        "# Load the dataset again without the column headers.\n",
        "df = pd.read_csv(file_path, header =None)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzlorlamNZUx"
      },
      "source": [
        "It seems like the first column might contain the serial numbers for the samples of glasses collected. Let's display the last 10 rows of the first column (indicated by 0) of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bC1LxOzmNRCw"
      },
      "source": [
        "#  Display the last 10 rows of the first column (indicated by 0) of the dataset.\n",
        "df[0].tail(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIj3Ru71N3p8"
      },
      "source": [
        "So our suspicion was correct. Let's drop this column because we don't need it to build a logistic regression model later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqFI0W5ZmWVS"
      },
      "source": [
        "# Drop the 0th column as it contains only the serial numbers.\n",
        "df.drop(columns = 0, inplace = True)\n",
        "\n",
        "# Get an array of the new set of columns.\n",
        "df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwWTbgSJOC-X"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhTI83OqOEXN"
      },
      "source": [
        "#### Activity 2: Renaming Column Headers\n",
        "\n",
        "Now let's provide the suitable column headers to the dataset so that we know the values of each independent variable for each glass sample. For this, we need to\n",
        "\n",
        "- Create a Python list containing the suitable column headers as string values. The desired column headers are `'RI', 'Na', 'Mg', 'Al', 'Si', 'K', 'Ca', 'Ba', 'Fe', 'GlassType'` in the same order.\n",
        "\n",
        "- Create a Python dictionary containing the current column heads and the desired column headers as key-value pairs.\n",
        "\n",
        "- Change the column heads by calling the `rename()` function of the `pandas` module on the `pandas` data frame object. The **syntax** to apply the `rename()` function is\n",
        "\n",
        "  `data_frame_object.rename(python_dictionary)`\n",
        "\n",
        "  where `python_dictionary` contains the elements as described in the second point.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_OJC96tNJ8d"
      },
      "source": [
        "#Create a Python list containing the suitable column headers as string values. Also, create a Python dictionary as described above.\n",
        "column_headers = ['RI', 'Na', 'Mg', 'Al', 'Si', 'K', 'Ca', 'Ba', 'Fe', 'GlassType']\n",
        "\n",
        "# Create the required Python dictionary.\n",
        "columns_dict = {}\n",
        "for i in df.columns:\n",
        "  columns_dict[i] = column_headers[i - 1]\n",
        "\n",
        "columns_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24yRw-92Nkhd"
      },
      "source": [
        "# Call the 'rename()' function on the data frame object to rename the columns.\n",
        "df.rename(columns_dict, axis = 1, inplace = True)\n",
        "\n",
        "# Display the first five rows of the data frame.\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifIb5K4ivsSs"
      },
      "source": [
        "As you can see, all the column headers are renamed as required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldnOLiTGsIhZ"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RH-ZA22wCrR"
      },
      "source": [
        "#### Activity 3: Dataset Inspection\n",
        "\n",
        "Let's look at the kind of values each of the columns have, number of rows and columns in the dataset and whether the dataset has any missing values or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gp9Ri0O3wO4y"
      },
      "source": [
        "#  Get the information about the dataset.\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZ1h1wm5wYsF"
      },
      "source": [
        "Except for the last column, all the columns have floating-point values as we already observed. There are 214 rows and 10 columns. And there are no missing values in the dataset because all the columns contain 214 non-null values.\n",
        "\n",
        "Now let's get the count of each glass-type samples in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sbx_H5rxisE"
      },
      "source": [
        "# Get the count of each glass-type samples in the dataset.\n",
        "df['GlassType'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKDiRKYHxv2K"
      },
      "source": [
        "Notice that there is no count for glass-type `4`. This means the dataset does not have any sample of glass-type `4`.\n",
        "\n",
        "Also, glass types `2` and `1` are the most common among all the samples and glass-type `6` is the least. This suggests that the dataset is slightly imbalanced and biased in-favour of types `1` and `2`. Let's also calculate the percentage of these values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKzWxq_IyJY3"
      },
      "source": [
        "# Get the percentage of count of each glass-type samples in the dataset.\n",
        "round(df['GlassType'].value_counts() * 100 / df.shape[0], 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xeFVCBByWXE"
      },
      "source": [
        "Through percentages, we can clearly see the imbalance in the dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXxgAXQ2zX3y"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_McfS4Oi3FaW"
      },
      "source": [
        "#### Activity 4: Data Visualisation using Plotly\n",
        "\n",
        "Plotly another Python library used for Data visualisation. We can create various kinds of graphs like line plot, pie plot, scatter plot etc. using plotly as well.  \n",
        "\n",
        "**So why should we use Plotly over matplotlib or seaborn?** The reason is:\n",
        "\n",
        "- There is a hover tool capabilities that can be use to observe anomalies in a large number of data points.\n",
        "\n",
        "- Also there are endless customizations to make interactive visualisation which can be displayed in Colab/Jupyter notebooks or standalone HTML files as well.\n",
        "\n",
        "\n",
        "Let's start with a creating a count plot using plotly.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Import the `plotly.express` module for Plotly features.\n",
        "\n",
        "2. Group the DataFrame `df` by the column `GlassType` without making it a default index column and save the grouping object in a variable .\n",
        "\n",
        "3. Compute the size of each group with the `size()` function.\n",
        "\n",
        ">`glass_group_df = glass_group.size()`\n",
        "\n",
        "> where `glass_group` is the grouping object and `size()` returns a DataFrame   the number of records in each unique group saved as `glass_group_df`.\n",
        "\n",
        "4.  Create the count plot with the `bar()` function of the plotly library. The syntax for the `bar()` function is:\n",
        "\n",
        "> **Syntax:**  `plotly.express.bar(data_frame, x, y, color)`\n",
        "\n",
        "> where\n",
        "\n",
        "  - `data_frame` : parameter requires the name of the dataframe with the distribution of values\n",
        "  - `x` : parameter requires a column name / pandas series name / array name from where the values are used to position marks along the x axis.\n",
        "\n",
        "  - `y` : parameter requires a column name / pandas series name / array name from where the values are used to position marks along the y axis.\n",
        "\n",
        "  - `color` : parameter requires a column name / pandas series name / array name from where the values are used to assign color to marks.\n",
        "\n",
        "5. Display the graph using the `show()` function.\n",
        "\n",
        "\n",
        "\n",
        "Let's create a count plot with plotly to observe the distribution of types of glasses in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4TQOzW3g5Do"
      },
      "source": [
        "# Create the count plot to observe distribution of glass types using Plotly.\n",
        "\n",
        "# Import the Plotly library\n",
        "import plotly.express as px\n",
        "\n",
        "# Group the DataFrame by the 'GlassType' column\n",
        "glass_group = df.groupby(by = \"GlassType\", as_index = False)\n",
        "\n",
        "# Get the size of each glass type from the group object\n",
        "glass_group_df = glass_group.size()\n",
        "print(glass_group_df.head())\n",
        "\n",
        "# Create the count plot using the 'bar()' function\n",
        "fig = px.bar(data_frame = glass_group_df, x = \"GlassType\", y = \"size\", color = \"GlassType\")\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "fig = px.colors.qualitative.swatches()\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "NcYW9NPYxk7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"GlassType\"].value_counts()"
      ],
      "metadata": {
        "id": "SvkUny3R-CMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express.colors\n",
        "a=df[\"GlassType\"].value_counts()\n",
        "a= pd.DataFrame(a )\n",
        "fig = px.bar(data_frame = a, x = a.index, y = \"count\", color=a.index)\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "6MDygetft3Cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upyMb8Hoo4TQ"
      },
      "source": [
        "**Note:** The `bar()` function can take in more parameters that can be passed to create more customised data. You may refer to the following document:\n",
        "\n",
        "https://plotly.com/python-api-reference/generated/plotly.express.bar.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkFZ6cQX1Bbv"
      },
      "source": [
        "As it can be observed, count plot is created using plotly. Also if you hover the mouse over the bars, a pop-up appears with the `GlassType` and its size information.\n",
        "\n",
        "We can also convert the plot to html with the `write_html()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAf536SL6-yf"
      },
      "source": [
        "#Convert the plot to html file.\n",
        "\n",
        "fig.write_html(\"Glass Distribution.html\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBs9oj188A0-"
      },
      "source": [
        "Check the file explorer on the left-hand side to verify if a new `.html` file is created. We can download that the graph file from the explorer.\n",
        "\n",
        "Now, let's move ahead create a scatter plot with Plotly with dummy data. The `plotly.express` has the function `scatter()` to create the scatter plot. The syntax of the `scatter()` function is:\n",
        "\n",
        "> **Syntax:**  `plotly.express.scatter(data_frame, x, y, color, size, hover_data, title)`\n",
        "\n",
        "> where\n",
        "\n",
        "\n",
        "Create a scatter plot and show the distribution across labels using the steps below:.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Create two NumPy arrays `x` and `y` with 10 integers from range 1-10 and 1-100 respectively.\n",
        "\n",
        "2. Create a NumPy array `labels` to divide the above array `data` into three labels - `1` , `2`, `3`  randomly.\n",
        "\n",
        "3. Create the scatter plot between `x` and `y` and show the distribution of data points with `labels` array with the color parameter.\n",
        "\n",
        "4. Display the plot.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4ocUpuIAWhO"
      },
      "source": [
        "#Create scatter plot between 'x' and 'y' and show the distribution using 'labels' array\n",
        "\n",
        "# Import the module\n",
        "import numpy as np\n",
        "\n",
        "# Create the 'x' and 'y' arrays\n",
        "x = np.random.randint(1,11,10, dtype = int)\n",
        "y = np.random.randint(1,101,10, dtype = int)\n",
        "\n",
        "# Create the 'labels' array\n",
        "labels = np.random.randint(1,3,10)\n",
        "\n",
        "print(f\"Array x: {x}\")\n",
        "print(f\"Array y: {y}\")\n",
        "print(f\"Labels array: {labels}\")\n",
        "\n",
        "# Create the scatter plot\n",
        "sc_plot = px.scatter(x = x, y = y, color = labels)\n",
        "\n",
        "# Display the plot\n",
        "sc_plot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cF6eRRzR3He"
      },
      "source": [
        "As it can be observed, the scatter plot is created with the dummy data points and different classes are assigned different colors. Also, when hover over the data points, the pop-up shows three pieces of information `x`, `y` and `color` which refers to the class of the data point.\n",
        "\n",
        "We can also observe the data points are really small. So, we can include the `size` parameter which should be an array of the same shape as values in `x`. The `size` parameter like `color` can be used to distinguish between different labels as well.\n",
        "\n",
        "Now, let's create a scatter plot using plotly between the column `Fe` to understand distribution of types of glasses with the Iron (Fe) with the guidlines below:\n",
        "\n",
        "- `dataframe` will be `df`\n",
        "\n",
        "- `x` will be an numpy array of size `df.shape[0]` within the range from the minimum value of the column `Fe` to the maximum value + 1 of the column `Fe`.\n",
        "\n",
        "- `y` will be the values in the column `Fe`\n",
        "\n",
        "- `size` will be values in the column `GlassType` such that the size of points change with the glass types\n",
        "\n",
        "- `color` will also be the values in the column `GlassType`such that the color of points change with the glass types.\n",
        "\n",
        "- `title` will be string representing the plot e.g. \"Scatter plot between Fe and Glass Type\"\n",
        "\n",
        "- `color_continuous_scale` will be `px.colors.sequential.Viridis`. This parameter is used to create list of continuous color scale values when the column denoted by `color` contains numeric data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVyidTHB1gwI"
      },
      "source": [
        "#Create the scatter plot for the column 'Fe' values and display the display the distribution of glass types over the column values.\n",
        "\n",
        "\n",
        "fig = px.scatter(df, x = np.linspace(df['Fe'].min(), df['Fe'].max() + 1, df.shape[0]), y = df['Fe'],\n",
        "                  size = \"GlassType\", color = \"GlassType\",\n",
        "                 color_continuous_scale = px.colors.sequential.Viridis, title = f'Scatter plot between Fe and Glass Type')\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNp64evLXus8"
      },
      "source": [
        "As it can be observed, the scatter plot is created for the column `Fe`. We can distinguish the data points into different types of glass using color with the color bar in the right or even the size (smallest represent label `1` and largest represent label `7`).\n",
        "\n",
        "The above scatter plot shows that label `5` type of glass can have the highest amout of `Fe`.\n",
        "\n",
        "**Note:** The different color scales can be observed in the `colors` sub modules of Plotly like `plotly.express.colors.sequential`, `plotly.express.colors.diverging` and `plotly.express.colors.cyclical`.\n",
        "\n",
        "Let's create the Plotly scatter plot for all the columns to check the distribution of glass types."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7XUn6cQlgmW"
      },
      "source": [
        "# Create the scatter plot for all the columns in 'df' to observe the distribution of glass types.\n",
        "\n",
        "for i in list(df.columns[:-1]):\n",
        "  fig = px.scatter(df, x= np.linspace(df[i].min(), df[i].max() + 1, df.shape[0]), y = df[i],\n",
        "                   hover_data = ['GlassType'], size = \"GlassType\",\n",
        "                   color = \"GlassType\", color_continuous_scale = px.colors.sequential.Viridis,\n",
        "                   title = f'Scatter plot between {i} and Glass Type')\n",
        "  fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tidAe4da_od"
      },
      "source": [
        "We can observe the scatter plots above to deduce various facts like `RI` reflective index of label `2` glass type is highest.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lsp4ug1OwCJq"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_w690ckUv0qQ"
      },
      "source": [
        "#### Activity 5: Model Building\n",
        "\n",
        "Let's build a logistic regression model first without balancing the dataset. If the model evaluation parameters suggest that the model is not classifying the labels correctly, then we will first deal with the imbalance and then build a logistic regression model again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0yVYJbqmLAZ"
      },
      "source": [
        "# Create separate data frames for training and testing the model.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Creating the features data frame holding all the columns accept last column\n",
        "x = df.iloc[:, :-1]\n",
        "print(f\"First five rows of the features data frame:\\n{x.head()}\\n\")\n",
        "\n",
        "# Creating the target series that holds last column 'GlassType'\n",
        "y = df['GlassType']\n",
        "print(f\"First five rows of the GlassType column:\\n{y.head()}\")\n",
        "\n",
        "# Splitting the train and test sets using the 'train_test_split()' function.\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odYfuRCo9tTg"
      },
      "source": [
        "#  Print the shape of all the four variables i.e. 'x_train', 'x_test', 'y_train' and 'y_test'\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IskflWOh_RHE"
      },
      "source": [
        "#  Build a logistic regression model using the 'sklearn' module.\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# 1. First, call the 'LogisticRegression' module and store it in 'lg_clg' variable.\n",
        "lg_clf = LogisticRegression()\n",
        "\n",
        "# 2. Call the 'fit()' function with 'x_train' and 'y_train' as inputs.\n",
        "lg_clf.fit(x_train, y_train)\n",
        "\n",
        "# 3. Call the 'score()' function with 'x_train' and 'y_train' as inputs to check the accuracy score of the model.\n",
        "lg_clf.score(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9hZWFpmjDIC"
      },
      "source": [
        "**Note:** This is a preliminary model building step. Hence, we can ignore `ConvergenceWarning` completely.  \n",
        "\n",
        "So the accuracy score is 61.75% which is not a good score.\n",
        "\n",
        "Now in the cases of binary classification, we generally create a confusion matrix and print the precision, recall and f1-score values. But in the case of multiclass classification, it best to first check what all labels the classification model identified or detected. For this, you can use either the `unique()` function or the `value_counts()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LY4yodj0jugK"
      },
      "source": [
        "# Get the target values predicted by the logistic regression model on the train set.\n",
        "y_train_predict = lg_clf.predict(x_train)\n",
        "y_train_predict = pd.Series(y_train_predict)\n",
        "\n",
        "print(\"Classes or labels identified by the logistic regression model:\\n\", y_train_predict.unique())\n",
        "print(\"\\nCount of the labels identified by the logistic regression model:\")\n",
        "print(y_train_predict.value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_uXnGZpk0M8"
      },
      "source": [
        "As you can see, the logistic regression model failed to identify glass-type `3`.\n",
        "\n",
        "Consequently, it does not makes sense to create a confusion matrix here because the actual target set has all the labels but the predicted target set misses one label (glass-type `3`) among the available (the whole dataset does not have any glass-type `4` sample) labels.\n",
        "\n",
        "Hence, **in the case of multiclass classification, before creating a confusion matrix, always first check whether the predicted target set has all the labels**.\n",
        "\n",
        "Let's repeat the above exercise on the test set and find out all the classes identified by the logistic regression model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlSKmfaDvDUU"
      },
      "source": [
        "#  Get the target values predicted by the logistic regression model on the test set.\n",
        "y_test_predict = pd.Series(lg_clf.predict(x_test))\n",
        "\n",
        "print(\"Classes or labels identified by the logistic regression model on the test set:\\n\", y_test_predict.unique())\n",
        "print(\"\\nCount of the labels identified by the logistic regression model on the test set:\")\n",
        "print(y_test_predict.value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMA-lJeNuqYu"
      },
      "source": [
        "On the test set, the logistic regression model failed to identify labels `3` and `6`. This is clearly a very bad classification model.\n",
        "\n",
        "Let's stop here. In the next class, we will try to build a logistic regression model again so that it can identify all the different labels before we can evaluate its performance further using confusion matrix, precision, recall and f1-score values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsdM85lGywxg"
      },
      "source": [
        "---"
      ]
    }
  ]
}